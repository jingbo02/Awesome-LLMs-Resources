# 多模态推理与强化学习：技术范式与未来展望

### 1. 核心趋势：为何“多模态+强化学习”成为推理能力跃迁的主轴

近期大量研究表明，单纯依赖监督微调（SFT）来模仿链式思维（CoT）存在显著瓶颈。强化学习（RL）通过引入更直接、可验证的反馈信号，正在成为推动多模态模型实现可泛化、结构化推理能力的关键。

- **SFT 的局限性**：
  - **伪推理与冗长**：模型学会了“看起来像”推理的语言模式，而非真正的逻辑推导，导致输出冗长空洞 ([VLAA-Thinking](https://arxiv.org/abs/2504.11468))。
  - **跨域脆弱性**：在特定领域微调后，模型在其他领域的推理能力下降 ([Vision-R1](https://arxiv.org/abs/2503.06749), [ReVisual-R1](https://arxiv.org/abs/2506.07516))。
  - **奖励信号错配**：SFT 的损失函数（如交叉熵）与最终任务目标（如准确率、IoU）并非完全对齐。
  - **灾难性遗忘**：持续的 SFT 可能导致模型遗忘在预训练阶段学到的通用能力 ([VLAA-Thinking](https://arxiv.org/abs/2504.11468))。
- **RL 的优势与抓手**：
  - **低成本硬反馈**：在数学、几何、代码、检测、分割等可被规则验证的任务上，RL 可以利用准确率、IoU、执行结果等作为直接的奖励信号，快速注入结构化推理能力 ([R1-VL](https://arxiv.org/abs/2503.12937), [Perception-R1](https://arxiv.org/abs/2504.07954), [VisionReasoner](https://arxiv.org/abs/2505.19651), [UniVG-R1](https://arxiv.org/abs/2505.14231), [SAM-R1](https://arxiv.org/abs/2505.22596))。
  - **对齐开放式任务**：对于创意写作、对话、复杂描述等开放式任务，RL 可以通过偏好模型、判别器或嵌入相似度等软奖励进行对齐 ([Omni-Thinker], [Mixed-R1](https://arxiv.org/abs/2505.24164), [R1-Omni], [Seed1.5-VL](https://arxiv.org/abs/2505.07062), [GLM-4.1V-Thinking](https://arxiv.org/abs/2507.01006))。
  - **催生“视觉内源思维”新范式**：RL 推动 CoT 超越了纯文本序列，发展出图文交错、像素级操作、生成中间视觉状态乃至纯视觉规划等新形态 ([Pixel Reasoner], [DeepEyes](https://arxiv.org/abs/2506.05943), [MINT-CoT](https://arxiv.org/abs/2506.05331), [GRIT](https://arxiv.org/abs/2505.15879), Thinking with Generated Images, [VPRL])。
- **训练范式的演进**：
  - **多元化序列**：传统的“SFT → RL”流程正在被更灵活的范式取代，如 RL→SFT ([Metis-RISE](https://arxiv.org/abs/2506.13056))、SFT↔RL 迭代 ([OpenVLThinker](https://arxiv.org/abs/2503.17352))、以及更复杂的跨模态强化学习序列 ([ReVisual-R1](https://arxiv.org/abs/2506.07516))。
  - **零冷启探索**：部分工作尝试完全不依赖 SFT，从零开始纯粹通过 RL 进行端到端训练 ([VisualThinker-R1-Zero](https://arxiv.org/abs/2503.05132), [DeepEyes](https://arxiv.org/abs/2506.05943))。
  - **自监督 RL**：利用模型自身的输出来构建伪奖励信号，实现无监督的自我提升 (MM-UPT)。
- **核心瓶颈转移**：随着 RL 的广泛应用，挑战也从“如何模仿”转向了更深层次的问题，如奖励劫持（Reward Hacking）、优势函数塌缩、样本难度失衡、多任务间的干扰 (MiMo-VL)、CoT 的质量控制 ([VLAA-Thinking](https://arxiv.org/abs/2504.11468)) 以及跨域迁移与遗忘的平衡 (Omni-Thinker) 等。

### 2. 总体范式

当前的多模态推理 RL 体系可抽象为一个五元组： **(Model, Data, Optimization, Reward, Reasoning Modality)**

- **Model (模型架构)**：
  - **基础骨架**：统一 Transformer 架构 + 高分辨率 ViT 成为主流 (MiMo-VL, [GLM-4.1V](https://arxiv.org/abs/2507.01006))。
  - **专家混合 (MoE)**：MoE 被用于提升模型容量与效率 (Seed1.5-VL, Kimi-VL)。
  - **连接器/投影器**：连接器的可训练性被发现是对齐视觉与语言模型的关键前提 ([Skywork-R1V3](https://arxiv.org/abs/2504.16656))。
  - **长上下文/记忆**：内插记忆或扩展上下文窗口以处理更复杂的推理链 ([GLM-4.1V 48K](https://arxiv.org/abs/2507.01006), [Seed1.5-VL 128K](https://arxiv.org/abs/2505.07062))。
- **Data (数据工程)**：
  - **混合类型**：结合多领域可验证任务数据（数学、代码）与开放域的偏好数据。
  - **难度分层/动态采样**：根据任务难度进行课程学习或动态采样，提升训练效率 (Curr-ReFT, Omni-Thinker, GLM-RLCS, [UniVG-R1](https://arxiv.org/abs/2505.14231), [ThinkLite-VL](https://arxiv.org/abs/2504.07934))。
  - **代表性数据集/方法**：MoDoMoDo, [ReVisual-R1 GRAMMAR](https://arxiv.org/abs/2506.07516), Mixed-45K, [GThinker](https://arxiv.org/abs/2506.01078), Vision-R1-cold。
- **Optimization (优化算法)**：
  - **GRPO 及其变体**：GRPO 家族因其稳定性成为主流选择，包括 StepGRPO、动态 KL 的 GRPO-D、多目标优化的 MORL、DAPO 等 (原始 GRPO、StepGRPO、GRPO-D、MORL、DAPO)。
  - **优势稳定技术**：SSR/SSB 缓存、PAD 优先优势蒸馏、非对称裁剪、令牌级标准化等技术被用于解决优势塌缩问题。
  - **其他算法**：PPO / 轻量级 PPO ([Open Vision Reasoner](https://arxiv.org/abs/2507.05255)) 和广义的 RLHF / RLAIF / RLVR 框架也被广泛应用 ([Mixed-R1](https://arxiv.org/abs/2505.24164), Seed1.5-VL, MiMo-VL)。
- **Reward (奖励设计)**：
  - **硬验证**：基于规则的奖励，如准确率 (Accuracy)、交并比 (IoU)、L1 距离、计数准确性、代码可执行性 (Executable) 等。
  - **结构/格式**：对输出的结构（如 JSON）、步骤完整性、逻辑性进行奖励。
  - **过程级**：对推理的每一步或特定阶段进行奖励 (StepGRPO)。
  - **偏好/混合**：使用判别器、偏好模型、嵌入相似度 (BMAS) 或多种奖励的加权混合 ([Mixed-R1](https://arxiv.org/abs/2505.24164), Omni-Thinker, MiMo-VL)。
  - **探索/正则**：引入好奇心探索、难度自适应、长度与冗余惩罚等。
- **Reasoning Modality (推理形态)**：
  - **从文本到多模态**：从纯文本 CoT 演进到图文交错 (iMCoT)、视觉元素标记 (Grounding)、像素级操作链 ([Pixel Reasoner])、工具调用 ([OpenThinkIMG](https://arxiv.org/abs/2505.08617))、视觉状态生成 (Thinking with Generated Images) 乃至纯视觉规划 ([VPRL])。

### 3. 任务类型与代表性工作

| 任务类别               | 代表性工作/模型                                              |
| ---------------------- | ------------------------------------------------------------ |
| **数学/几何**          | [Vision-R1](https://arxiv.org/abs/2503.06749), [MINT-CoT](https://arxiv.org/abs/2506.05331), [Metis-RISE](https://arxiv.org/abs/2506.13056), [ThinkLite-VL](https://arxiv.org/abs/2504.07934), [ReVisual-R1](https://arxiv.org/abs/2506.07516), [GLM-4.1V](https://arxiv.org/abs/2507.01006), [GThinker](https://arxiv.org/abs/2506.01078) |
| **代码/程序**          | Omni-Thinker, MiMo-VL                                        |
| **视觉定位/检测/计数** | [Perception-R1](https://arxiv.org/abs/2504.07954), [Vision-R1](https://arxiv.org/abs/2503.06749), VLM-R1, [VisionReasoner](https://arxiv.org/abs/2505.19651), [UniVG-R1](https://arxiv.org/abs/2505.14231), [GRIT](https://arxiv.org/abs/2505.15879), [SAM-R1](https://arxiv.org/abs/2505.22596), [Pixel Reasoner] |
| **分割**               | [SAM-R1](https://arxiv.org/abs/2505.22596), ReasonSeg        |
| **图表/文档/OCR**      | [OpenThinkIMG](https://arxiv.org/abs/2505.08617), [VisionReasoner](https://arxiv.org/abs/2505.19651), [Seed1.5-VL](https://arxiv.org/abs/2505.07062), MiMo-VL, [GLM-4.1V](https://arxiv.org/abs/2507.01006), [DeepEyes](https://arxiv.org/abs/2506.05943) |
| **工具增强**           | [OpenThinkIMG](https://arxiv.org/abs/2505.08617), [VisTA](https://arxiv.org/abs/2505.20289), [Pixel Reasoner], [DeepEyes](https://arxiv.org/abs/2506.05943) |
| **视觉思维链与再思考** | [VL-Rethinker](https://arxiv.org/abs/2504.08837), [GThinker](https://arxiv.org/abs/2506.01078), [D2I](https://arxiv.org/abs/2507.06999), [OpenVLThinker](https://arxiv.org/abs/2503.17352), [Virgo](https://arxiv.org/abs/2501.01904) |
| **规划/纯视觉状态**    | Visual Planning ([VPRL]), Vision Agent GUI ([GLM-4.1V](https://arxiv.org/abs/2507.01006), Seed1.5-VL, Kimi-VL) |
| **情感 & 音视频**      | R1-Omni, Seed1.5-VL                                          |
| **无监督/自监督**      | MM-UPT, [ThinkLite-VL](https://arxiv.org/abs/2504.07934), [VisualThinker-R1-Zero](https://arxiv.org/abs/2503.05132) |

> 注：ReasonSeg 可参见 LISA 工作（CVPR 2024）中的 reasoning segmentation 设定与数据：PDF 链接见下文“引用汇总”。

### 4. 推理思维链的形态演进

1. **纯文本 CoT**：`问题 -> 思考步骤 (纯文本) -> 答案`。
2. **图文交错 CoT**：在文本链中插入图像或图像 Token ([iMCoT], [GRIT](https://arxiv.org/abs/2505.15879))。
3. **结构化视觉标记**：思维链直接输出结构化的视觉标记，如边界框、掩码或点坐标 ([UniVG-R1](https://arxiv.org/abs/2505.14231), [VisionReasoner](https://arxiv.org/abs/2505.19651), [SAM-R1](https://arxiv.org/abs/2505.22596))。
4. **像素级原生感知操作**：将 ZOOM、FRAME SELECT 等像素级操作作为推理步骤 ([Pixel Reasoner], [DeepEyes](https://arxiv.org/abs/2506.05943))。
5. **工具/程序外化**：通过调用外部工具或执行代码来辅助推理 ([OpenThinkIMG](https://arxiv.org/abs/2505.08617), [VisTA](https://arxiv.org/abs/2505.20289), [VPRL])。
6. **内在生成视觉思维**：模型生成中间图像来表示子目标或进行自我批评 (Thinking with Generated Images)。
7. **线索-反思循环**：基于视觉线索进行反复检查和修正的循环推理 ([GThinker](https://arxiv.org/abs/2506.01078), [VL-Rethinker](https://arxiv.org/abs/2504.08837), [D2I](https://arxiv.org/abs/2507.06999))。
8. **跨模态行为迁移**：将从文本推理中学到的“慢思维”或行为模式迁移到视觉任务中 ([Open Vision Reasoner](https://arxiv.org/abs/2507.05255), [Virgo](https://arxiv.org/abs/2501.01904))。
9. **情感与多流信号耦合**：融合音频、视频等多模态信号进行综合推理 (R1-Omni)。
10. **通用统一多域链式 & 规划**：在一个统一框架下处理多领域、多任务的复杂推理与规划 ([GLM-4.1V](https://arxiv.org/abs/2507.01006), [Seed1.5-VL](https://arxiv.org/abs/2505.07062), MiMo-VL, Omni-Thinker)。

### 5. 训练范式

- **SFT 的必要性之辩**：
  - **“SFT有害”论**：SFT 诱导模型学习伪 CoT，直接 RL 或先 RL 后 SFT 效果更好 ([VLAA-Thinking](https://arxiv.org/abs/2504.11468), [VisualThinker-R1-Zero](https://arxiv.org/abs/2503.05132), [Metis-RISE](https://arxiv.org/abs/2506.13056))。
  - **“SFT必要”论**：高质量 SFT 数据对于模型冷启动、学习基本格式和调用语法至关重要 ([UniVG-R1](https://arxiv.org/abs/2505.14231), [SAM-R1](https://arxiv.org/abs/2505.22596), [GRIT](https://arxiv.org/abs/2505.15879), [OpenThinkIMG](https://arxiv.org/abs/2505.08617))。
  - **折衷/迭代方案**：SFT 与 RL 可以迭代进行 ([OpenVLThinker](https://arxiv.org/abs/2503.17352))，或采用多阶段训练，先在文本域 RL，再迁移到多模态域 ([ReVisual-R1](https://arxiv.org/abs/2506.07516))。
- **多阶段/课程学习**：
  - **难度排序**：根据任务复杂度或模型遗忘率安排训练顺序 (Curr-ReFT, Omni-Thinker)。
  - **动态采样**：自适应地选择训练样本的难度 (GLM-RLCS, [ThinkLite-VL](https://arxiv.org/abs/2504.07934))。
  - **分阶段训练**：将训练分为不同目标（如常识、多模态、文本）的多个阶段 ([ReVisual-R1](https://arxiv.org/abs/2506.07516))。
- **统一多任务 RL**：
  - **多目标优化 (MORL)**：同时优化来自不同任务的奖励信号 (MiMo-VL)。
  - **混合奖励/数据**：将不同任务的数据和奖励函数混合训练 ([Mixed-R1](https://arxiv.org/abs/2505.24164))。
  - **权重优化**：使用进化算法等方法自动调节不同任务的权重 (MoDoMoDo)。
- **非监督/自监督**：
  - **自投票/伪标签**：通过多数投票等方式生成伪奖励信号 (MM-UPT)。
  - **自蒸馏**：从模型自身生成的更优轨迹中学习 ([Metis-RISE](https://arxiv.org/abs/2506.13056), [Virgo](https://arxiv.org/abs/2501.01904))。
  - **轨迹合成/过滤**：合成潜在的推理路径并过滤出有效的用于训练 ([OpenThinkIMG](https://arxiv.org/abs/2505.08617))。

### 6. 奖励设计与信号工程

| 层次                   | 类型               | 具体方法与实例                                               |
| ---------------------- | ------------------ | ------------------------------------------------------------ |
| **I. 基础可验证**      | 正确性、空间、计数 | 答案匹配 (Omni-Thinker), IoU ([VisionReasoner](https://arxiv.org/abs/2505.19651)), 数值一致性 ([Perception-R1](https://arxiv.org/abs/2504.07954)), 分割 IoU ([SAM-R1](https://arxiv.org/abs/2505.22596))。 |
| **II. 结构与格式**     | JSON/模板/语法     | `<think> / <answer>` 格式奖励 (StepRVR), 非重复奖励 ([VisionReasoner](https://arxiv.org/abs/2505.19651)), 条件工具奖励 ([DeepEyes](https://arxiv.org/abs/2506.05943))。 |
| **III. 步骤级/过程级** | 按步骤奖励/控制    | StepGRPO (StepRAR/StepRVR)（参见 [R1-VL](https://arxiv.org/abs/2503.12937)）, 长度控制 ([Vision-R1 PTST](https://arxiv.org/abs/2503.06749)), 强制再思考触发 ([VL-Rethinker](https://arxiv.org/abs/2504.08837)), 线索回溯 ([GThinker](https://arxiv.org/abs/2506.01078))。 |
| **IV. 复杂合成/混合**  | 多信号加权/组合    | 混合奖励+BMAS 相似度 ([Mixed-R1](https://arxiv.org/abs/2505.24164)), MORL (RLVR+RLHF) (MiMo-VL), 多域独立奖励器 ([GLM-4.1V](https://arxiv.org/abs/2507.01006))。 |
| **V. 探索激励**        | 好奇心/难度/防塌缩 | 好奇心奖励 ([Pixel Reasoner]), 难度权重 ([UniVG-R1](https://arxiv.org/abs/2505.14231)), MCTS 难度选择 ([ThinkLite-VL](https://arxiv.org/abs/2504.07934)), SSR/SSB/PAD 对抗优势塌缩（SSB 思路见 [Skywork 系列报告](https://arxiv.org/abs/2504.16656)）。 |
| **VI. 条件/组合**      | 依赖特定行为       | 答对且使用工具才加分 ([DeepEyes](https://arxiv.org/abs/2506.05943)), 根据工具效果给予正/负/零奖励 ([VisTA](https://arxiv.org/abs/2505.20289))。 |
| **VII. 反幻觉与鲁棒**  | 监控/惩罚伪链      | 奖励熵监控 ([Skywork-R1V3](https://arxiv.org/abs/2504.16656)), 惩罚冗余预测 (VLM-R1 odLength), 无 CoT 直接定位奖励 ([Perception-R1](https://arxiv.org/abs/2504.07954))。 |

### 7. 数据工程与难度管理

- **课程化与难度度量**：使用遗忘率 (BWT)、错误率、MCTS 迭代数、GPT 评分等指标对数据进行分级 ([OpenVLThinker](https://arxiv.org/abs/2503.17352))。
- **混合优化**：通过二次性能代理 + CMA-ES (MoDoMoDo) 或多阶段预训练 + RL (MiMo-VL, Seed1.5-VL) 来平衡多任务数据。
- **高质量 CoT 构建**：采用人工审核 ([UniVG-R1](https://arxiv.org/abs/2505.14231))、GPT-4o 分解+过滤 ([OpenThinkIMG](https://arxiv.org/abs/2505.08617))、模板合成 ([Pixel Reasoner]) 等方法提升 CoT 数据质量。
- **自蒸馏/伪标签**：利用多响应投票 (MM-UPT)、专家增强自蒸馏 ([Metis-RISE](https://arxiv.org/abs/2506.13056)) 等方式扩充数据。
- **难度偏差矫正**：通过难度加权 (-mIoU, [UniVG-R1](https://arxiv.org/abs/2505.14231))、聚焦中等难度样本 ([ThinkLite-VL](https://arxiv.org/abs/2504.07934))、渐进式释放任务复杂度 (Curr-ReFT) 来优化学习曲线。

### 8. 策略优化与稳定性技术

- **基础算法**：GRPO / PPO / StepGRPO / DAPO（参见 [R1-VL](https://arxiv.org/abs/2503.12937) 与 [Open Vision Reasoner](https://arxiv.org/abs/2507.05255)）。
- **优势塌缩解决方案**：
  - **SSR (Stochastic Successive Rejection)**：缓存非零优势的样本 ([VL-Rethinker](https://arxiv.org/abs/2504.08837))。
  - **SSB (Selective Successive Buffer)**：优先采样高价值样本（见 [Skywork-R1V2/3 技术报告](https://arxiv.org/abs/2504.16656)）。
  - **PAD (Priority Advantage Distillation)**：过滤零优势样本并重加权 ([ReVisual-R1](https://arxiv.org/abs/2506.07516))。
  - **其他**：令牌级归一化、非对称裁剪 ([SAM-R1](https://arxiv.org/abs/2505.22596))、动态 KL (GRPO-D)、熵监控 ([Skywork-R1V3](https://arxiv.org/abs/2504.16656))。
- **多任务干扰缓解**：
  - **任务排序**：按特定顺序训练任务以减少遗忘 (Omni-Thinker)。
  - **动态采样**：动态平衡不同领域的采样率 (GLM RLCS)。
  - **权重求解**：通过优化算法求解任务权重 ([Mixed-R1](https://arxiv.org/abs/2505.24164), MoDoMoDo)。
- **连接器再调优**：在 RL 阶段冻结 ViT 和 LLM，仅微调连接器，以防止知识漂移并实现更稳定的对齐 ([Skywork-R1V3](https://arxiv.org/abs/2504.16656))。

### 9. 工具、像素、程序与规划扩展层

- **工具策略学习**：通过 RL 学习何时以及如何调用外部工具 ([OpenThinkIMG](https://arxiv.org/abs/2505.08617), [VisTA](https://arxiv.org/abs/2505.20289))。
- **像素操作作为内生工具**：将缩放、帧选择等像素级操作作为模型的“内在工具”，通过 RL 学习其使用策略 ([Pixel Reasoner], [DeepEyes](https://arxiv.org/abs/2506.05943))。
- **Grounding 输出**：将视觉定位（bounding box）作为 CoT 的一部分直接输出，实现语言与视觉的紧密链接 ([GRIT](https://arxiv.org/abs/2505.15879), [UniVG-R1](https://arxiv.org/abs/2505.14231))。
- **程序/视觉规划**：通过 RL 学习生成一系列纯视觉状态或操作序列来解决规划问题 ([VPRL], Visual Planning)。
- **统一感知与推理**：在一个框架内统一检测、分割、计数等多种感知任务，并通过 RL 进行多目标优化 ([VisionReasoner](https://arxiv.org/abs/2505.19651), [Perception-R1](https://arxiv.org/abs/2504.07954), [SAM-R1](https://arxiv.org/abs/2505.22596))。

### 10. 自反思、再思考与思维控制

- **强制反思机制**：在推理链末尾注入触发词，或设置特定规则来强制模型进行自我检查和修正 ([VL-Rethinker](https://arxiv.org/abs/2504.08837), [GThinker](https://arxiv.org/abs/2506.01078), [D2I](https://arxiv.org/abs/2507.06999))。
- **轨迹筛选与截断**：去除重复的思考循环，警惕“伪 aha”时刻的无效推理路径 ([OpenVLThinker](https://arxiv.org/abs/2503.17352), [VLAA-Thinking](https://arxiv.org/abs/2504.11468))。
- **视觉反思行为定义**：将视觉搜索、比较、确认、幻觉缓解等行为模式化，作为模型可学习的策略 ([Open Vision Reasoner](https://arxiv.org/abs/2507.05255), [DeepEyes](https://arxiv.org/abs/2506.05943))。

### 11. 迁移与跨域泛化机制

- **文本 → 视觉迁移**：先在文本数据上通过 RL 建立抽象逻辑能力，再将其迁移到多模态任务中 ([Virgo](https://arxiv.org/abs/2501.01904), LMM-R1, [ReVisual-R1](https://arxiv.org/abs/2506.07516))。
- **视觉 → 其他任务迁移**：在某一视觉任务（如几何）上学到的推理能力可以显著提升其他任务（如计数）的性能 (OThink-MR1)。
- **结构化 → 开放式迁移**：通过混合奖励（硬验证 + 相似度/判别器）来将在结构化任务上学到的能力泛化到开放式描述任务中 ([Mixed-R1](https://arxiv.org/abs/2505.24164), Omni-Thinker)。

### 12. 资源效率与小样本策略

- **极低样本成功案例**：GRIT (20), [SAM-R1](https://arxiv.org/abs/2505.22596) (3K), [VisionReasoner](https://arxiv.org/abs/2505.19651) (7K), [ThinkLite-VL](https://arxiv.org/abs/2504.07934) (11K), MM-UPT (0 标注)。
- **关键策略**：
  - **数据高效**：难度过滤、多数投票伪标签、离线 MCTS 预筛选。
  - **模型高效**：连接器局部调优（LoRA-like）。
  - **奖励高效**：好奇心奖励扩大探索、结构化格式奖励压缩标注需求。

### 13. 评测体系与缺口

- **当前现状**：主要依赖答案级别的指标（Accuracy, mAP, IoU, Pass@k），缺乏对推理过程的评估。
- **主要缺口**：
  - **过程忠实度指标**：缺少标准化的指标来衡量文本步骤与视觉证据的一致性 (Step Faithfulness, Visual Grounding Consistency)。
  - **幻觉诊断**：对模型幻觉的系统性评估和诊断尚不完善。
  - **安全与稳健性**：针对对抗性攻击、噪声、GUI 多样性等的 RL 研究较少。
  - **基准碎片化**：各项能力（数学、视觉、工具）的评测基准相互独立，缺少综合性场景。

### 14. 对立观点与经验法则

- **“SFT vs RL” 顺序**：
  - **直接 RL**：避免 SFT 引入的伪推理链 ([VLAA-Thinking](https://arxiv.org/abs/2504.11468))。
  - **RL → SFT**：RL 激发潜力，SFT 精准补全短板 ([Metis-RISE](https://arxiv.org/abs/2506.13056))。
  - **SFT 冷启**：高质量 SFT 对于稳定性和格式化仍然有益 ([UniVG-R1](https://arxiv.org/abs/2505.14231))。
- **“显式 CoT” 必要性**：
  - **对感知可能有害**：对于直接的感知定位任务，CoT 可能干扰性能 ([Perception-R1](https://arxiv.org/abs/2504.07954))。
  - **对抽象推理有益**：对于数学等多步抽象推理，CoT + RL 增益明显 ([Metis-RISE](https://arxiv.org/abs/2506.13056))。
- **“链越长越好？”**：
  - **并非如此**：存在最优的推理长度窗口，过长或过短都可能损害性能 ([Virgo](https://arxiv.org/abs/2501.01904), [Vision-R1 PTST](https://arxiv.org/abs/2503.06749))。
- **“多任务必然共赢？”**：
  - **可能存在干扰**：不同任务（如长链推理 vs 短输出定位）之间可能存在冲突，需要通过课程学习或权重优化来解决 (MiMo-VL, MoDoMoDo)。

### 15. 代表性方法分类

| 类别                  | 代表性方法/论文                                              |
| --------------------- | ------------------------------------------------------------ |
| **纯 RL / 零 SFT**    | [VisualThinker-R1-Zero](https://arxiv.org/abs/2503.05132), [DeepEyes](https://arxiv.org/abs/2506.05943), [Pixel Reasoner], MM-UPT |
| **迭代/循环训练**     | [OpenVLThinker](https://arxiv.org/abs/2503.17352), [ReVisual-R1](https://arxiv.org/abs/2506.07516), LMM-R1 |
| **混合奖励/高维优化** | Omni-Thinker, [Mixed-R1](https://arxiv.org/abs/2505.24164), MiMo-VL, [GLM-4.1V](https://arxiv.org/abs/2507.01006), Seed1.5-VL, R1-Omni, [OpenThinkIMG](https://arxiv.org/abs/2505.08617) |
| **难度策略/课程学习** | [ThinkLite-VL](https://arxiv.org/abs/2504.07934), Curr-ReFT, GLM RLCS, Omni-Thinker, [UniVG-R1](https://arxiv.org/abs/2505.14231), [ReVisual-R1](https://arxiv.org/abs/2506.07516) |
| **视觉内源思维**      | [GRIT](https://arxiv.org/abs/2505.15879), [MINT-CoT](https://arxiv.org/abs/2506.05331), [DeepEyes](https://arxiv.org/abs/2506.05943), [Pixel Reasoner], Thinking with Generated Images, [VPRL] |
| **感知统一框架**      | [Perception-R1](https://arxiv.org/abs/2504.07954), [VisionReasoner](https://arxiv.org/abs/2505.19651), [SAM-R1](https://arxiv.org/abs/2505.22596), [Vision-R1](https://arxiv.org/abs/2503.06749) |
| **反思/再思考增强**   | [VL-Rethinker](https://arxiv.org/abs/2504.08837), [GThinker](https://arxiv.org/abs/2506.01078), [D2I](https://arxiv.org/abs/2507.06999), [Metis-RISE](https://arxiv.org/abs/2506.13056), [Virgo](https://arxiv.org/abs/2501.01904) |

### 16. 关键技术模式

- **双阶段差异化训练**：抽象逻辑先行（文本 RL），感知对齐跟上（多模态 RL），最后语言精炼（文本 SFT/RL）。
- **难度曲线控制**：聚焦中等难度样本以保持有效梯度，同时通过动态 KL 或递进奖励阈值来平衡探索与利用。
- **过程密集化奖励**：通过步骤级奖励和细粒度的视觉 grounding 奖励，提供更密集的监督信号。
- **视觉操作内化**：将外部工具或操作（如缩放）内化为模型的可学习策略。
- **混合验证层**：结合硬性规则、模型判别器和嵌入相似度，实现对复杂开放式任务的有效评估。
- **反幻觉机制**：通过条件奖励、显式 grounding 和熵监控等手段抑制幻觉。
- **资源效率**：离线评估、局部微调和伪标签生成是实现高效训练的关键。

### 17. 当前主要瓶颈

- **统一过程评估缺失**：缺乏标准化的“视觉证据-文本步骤”忠实度指标。
- **奖励 Hack 与过拟合**：模型易于找到奖励函数的捷径，产出形式正确但内容空洞的推理。
- **多任务干扰与负迁移**：不同任务的梯度方向或奖励尺度可能冲突。
- **幻觉与伪视觉引用**：模型可能“捏造”图像中不存在的细节。
- **自反思的真实性**：难以区分模型是真正进行反思还是在模仿反思的模板。
- **数据合成的真实性**：合成数据可能带有系统性偏差。
- **工具/像素操作的扩展性**：当前可用的操作集仍然很窄，缺乏通用性。

### 18. 未来方向与研究机会

1. **过程级可信评估**：开发步骤级的多模态忠实度指标。
2. **图结构推理奖励**：将奖励从线性链扩展到有向无环图（DAG）结构。
3. **统一视觉操作 DSL**：定义一种视觉操作的领域特定语言，让 RL 策略学习其组合。
4. **世界模型与主动感知**：将推理扩展到 3D 和时序仿真中。
5. **融合不确定性估计**：在奖励中引入置信度校准，惩罚高置信度的错误。
6. **端到端自举闭环**：实现无 SFT 纯 RL + 自蒸馏 + 自动合成的全闭环。
7. **奖励自动化探测**：利用可学习的奖励模型减少人工设计成本。
8. **跨模态记忆与检索**：缓存并复用视觉推理状态。
9. **绿色高效推理**：研究自适应提前终止、Token 剪枝等高效策略。
10. **隐私与安全**：在 RL 框架中引入伦理和安全约束。

### 19. 实践落地设计建议

- **数据与课程**：混合可验证任务与开放任务，按难度排序，聚焦中等难度，并保留失败样本用于优势回放。
- **奖励工程**：构建分层奖励栈（基础验证 → 结构 → 过程 → 探索 → 开放式），并加入反 Hack 策略。
- **优化策略**：优选 GRPO 变体，监控优势分布，谨慎使用 SFT 进行冷启动。
- **视觉思维与工具**：从简单的 grounding 开始，逐步引入像素操作和轻量级工具接口。
- **反思与再审**：仅对困难样本启用强制反思，避免模板化。
- **泛化与稳定**：采用多阶段训练范式，并使用高级优化策略调配多任务权重。
- **监控仪表**：跟踪关键思维 Token 熵、奖励组成、链长度、工具调用频率等指标。

### 20. 结论

多模态推理的强化学习正经历一场深刻的范式转变，从简单的“文本链模仿”演进为集“视觉内源思维、工具/像素操作、策略反思”于一体的综合智能阶段。这一演进的核心驱动力源于**奖励信号的结构化**、**推理形态的视觉化**以及**训练范式的动态化**。尽管在过程评估、奖励 Hack、多任务平衡等方面仍面临挑战，但未来的研究方向明确指向了更可信、通用和高效的原生多模态推理模型。这要求在奖励、数据、优化和思维模式上进行协同设计，以突破当前的瓶颈。



**Thanks to https://www.zhihu.com/people/zai-zhe-teng-liang-nian-ba**