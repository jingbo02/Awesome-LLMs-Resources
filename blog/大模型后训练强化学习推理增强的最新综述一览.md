# 大模型后训练/强化学习/推理增强的最新综述一览

------

## 共同主题与脉络

- **从“预训练规模”转向“后训练与推理规模”**：多篇论文都指出仅靠扩大预训练已逼近边际收益，研究焦点正在转向**监督微调（SFT）**、**来自反馈的强化学习（RLxF/RLHF/RLAIF 等）**以及**推理时计算（Test-time Compute）**等后训练与推理阶段的“可扩展性”路径。
- **强化学习贯穿全链路**：不仅用于对齐，还用于**数据生成**、**预训练目标改造**、**后训练优化**以及**推理期搜索/规划**（如 MCTS、ToT/GoT）。([SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5128927))
- **方法论与分类体系逐步成型**：技术综述系统梳理了**PPO/Actor-Critic/Q-learning**、**RLHF/RLAIF/DPO/GRPO**、**验证器驱动训练/结果式奖励**等方法族，并总结其优势、风险与可扩展瓶颈。([arXiv](https://arxiv.org/abs/2507.04136))
- **从“会说话”到“会行动”的代理化趋势**：最新的“Agentic RL”综述强调把 LLM 置于**POMDP/长期决策**框架中，围绕**规划、工具使用、记忆、推理、自改进、感知**等能力构建统一图谱与基准生态。([arXiv](https://arxiv.org/abs/2509.02547))
- **“大推理模型（LRM）”的兴起**：将**训练期的强化学习式“学习-推理”\**与\**测试期的思维链扩展/搜索**联合规模化，被视为通往更强推理能力的关键路线；OpenAI o1 系列被视作这一方向的重要里程碑之一。([arXiv](https://arxiv.org/abs/2501.09686))

------

## 论文逐篇概述（含链接与PDF）

### 1) A Survey of Post-Training Scaling in Large Language Models

- **页面**：ACL Anthology（长文）｜亦有 OpenReview 页面
  - PDF（ACL）: https://aclanthology.org/2025.acl-long.140.pdf
  - 页面（OpenReview）: https://openreview.net/forum?id=DeDCqFUajk
- **出版/版本**：ACL 2025 长文（7月27日–8月1日）
- **核心内容**：系统化提出“**后训练规模化**”范式：将传统训练中占比很小的**对齐阶段**规模化，归纳为三大支柱——**SFT、RLxF（广义来自反馈的强化学习）、Test-time Compute（推理时算力/策略）**，并给出趋势、应用场景（数学、代码、代理等）与未解决问题（数据自动化、奖励可扩展性、推理成本等）。

> 引用依据：论文明确把后训练规模化拆为 SFT / RLxF / TTC，并与预训练扩展进行比较。([OpenReview](https://openreview.net/forum?id=DeDCqFUajk&utm_source=chatgpt.com))

------

### 2) A Survey of Reinforcement Learning in Large Language Models: From Data Generation to Test-Time Inference

- **页面（SSRN）**: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5128927
- **PDF（SSRN 打开）**: （页面的 “Open PDF in Browser” 按钮）https://papers.ssrn.com/sol3/Delivery.cfm/5128927.pdf?abstractid=5128927&mirid=1&type=2
- **日期**：撰写于 2025-01-31，SSRN 发布 2025-04-07。([SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5128927))
- **核心内容**：将 RL 在 LLM 生命周期的角色分为**四阶段**：**数据生成 → 预训练 → 后训练 → 测试时推理**；覆盖**搜索驱动推理**（MCTS、Tree/Graph-of-Thought）与**规划驱动推理**（以世界模型视角决策），讨论扩展难点（奖励设计/稳定性/成本）与未来方向（混合式 RL、可扩展反馈框架）。([SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5128927))

------

### 3) A Technical Survey of Reinforcement Learning Techniques for Large Language Models

- **页面（arXiv）**: https://arxiv.org/abs/2507.04136
- **PDF**: https://arxiv.org/pdf/2507.04136
- **提交时间**：2025-07-05（v1）。([arXiv](https://arxiv.org/abs/2507.04136))
- **核心内容**：偏**技术细节/算法视角**的 RL×LLM 综述；系统讲解**PPO / Actor-Critic / Q-learning**等基础 RL 及其在 LLM 对齐中的落地；覆盖**RLHF / RLAIF / DPO / GRPO**、**验证器/结果导向 RL（如 RLVR）**、**多目标对齐**等；总结痛点（奖励欺骗、成本、可扩展反馈）与前沿（验证器引导、混合 RL）。([arXiv](https://arxiv.org/abs/2507.04136))

------

### 4) The Landscape of Agentic Reinforcement Learning for LLMs: A Survey

- **页面（arXiv）**: https://arxiv.org/abs/2509.02547
- **PDF**: https://arxiv.org/pdf/2509.02547
- **提交时间**：2025-09-02（v1）。([arXiv](https://arxiv.org/abs/2509.02547))
- **核心内容**：提出将“LLM-RL 的单步 MDP 视角”提升为**面向环境与长期决策的 POMDP/代理范式**；从**能力视角**（**规划、工具用、记忆、推理、自改进、感知**等）与**应用视角**构建双重分类，并汇总**开放环境/基准/框架**生态，强调 RL 是把这些能力从启发式插件变成**自适应、鲁棒代理行为**的关键机制。([arXiv](https://arxiv.org/abs/2509.02547))

------

### 5) Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models

- **页面（arXiv）**: https://arxiv.org/abs/2501.09686
- **PDF**: https://arxiv.org/pdf/2501.09686
- **版本**：首提 2025-01-16，修订至 2025-01-23（v3）。([arXiv](https://arxiv.org/abs/2501.09686))
- **核心内容**：以“**强化的推理（Reinforced Reasoning）**”为主线，串联**自动化推理轨迹生成（试错搜索）\**的训练期扩展与\**测试期思维链扩展/搜索**（让模型“想得更久”）；提出**联合放大量的训练期与测试期推理**可通往“**大推理模型（LRM）**”；文中把 **OpenAI o1** 作为重要标志案例之一，并梳理开源实践与开放挑战。([arXiv](https://arxiv.org/abs/2501.09686))

------

## 横向对比

| 维度     | Post-Training Scaling Survey                       | RL in LLMs（全链路）           | RL 技术综述（算法向）                  | Agentic RL（代理范式）                    | LRM（强化推理）                           |
| -------- | -------------------------------------------------- | ------------------------------ | -------------------------------------- | ----------------------------------------- | ----------------------------------------- |
| 覆盖范围 | 后训练与推理期的**规模化三支柱：SFT / RLxF / TTC** | **数据→预训→后训→推理**四阶段  | RL×LLM 的**算法/优化技术细节**         | **POMDP/长期决策**下的代理能力与生态      | **训练期强化学习 + 测试期思维扩展**的结合 |
| 关注重心 | 如何把对齐与推理**规模化**                         | RL 在 LLM **全生命周期**的角色 | **PPO/DPO/GRPO/RLHF/RLAIF** 等方法脉络 | **规划/工具/记忆/自改进**等能力图谱与基准 | “**想更久**”与**轨迹学习**如何提升推理    |
| 方法代表 | RLxF、Self-verify、ToT/搜索、TTC 策略              | MCTS、ToT/GoT、规划式推理      | RLVR/验证器驱动、结果式奖励、混合 RL   | 代理式 RL 训练、环境交互/评测套件         | 试错生成推理轨迹、测试时多样化采样/搜索   |
| 产出形态 | 综述 + 分类图谱 + 未解问题                         | 综述 + 生命周期框架            | 综述 + 技术对照与趋势                  | 综述 + 能力/应用双分类 + 生态汇编         | 综述 + “LRM” 路线图                       |
| 读者对象 | 策略/落地与趋势跟踪                                | 端到端视角的研究/工程          | 需要**算法细节**与**优化技巧**的读者   | 研究**自治代理**与复杂任务的人群          | 研究**推理能力**与**o1/R1**式路线的人群   |

> 注：以上定位分别依据各论文的结构与摘要重点而来（见前述各条目下的引用）。([SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5128927), [arXiv](https://arxiv.org/abs/2507.04136))

------

### 总结

这些综述从不同角度指向同一趋势：**把“对齐/强化/推理”做成可扩展工程**。无论是把**后训练与推理时计算**当作新的“规模律”，还是用 RL 贯穿**数据-训练-对齐-推理**的全流程，抑或在**代理化**与**大推理模型**两条路线中升级“思考的深度与广度”，核心都是**让模型在更多反馈、更强搜索、更长推理链条中自我改进**，并形成**标准化的评测/生态**来闭环演进。([SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5128927), [arXiv](https://arxiv.org/abs/2507.04136))

------

**参考链接（便于一键访问）**

- Post-Training Scaling（ACL 2025 PDF）：https://aclanthology.org/2025.acl-long.140.pdf ；OpenReview： https://openreview.net/forum?id=DeDCqFUajk ([OpenReview](https://openreview.net/forum?id=DeDCqFUajk&utm_source=chatgpt.com))
- RL in LLMs（SSRN 页面 / PDF）：https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5128927 ｜ https://papers.ssrn.com/sol3/Delivery.cfm/5128927.pdf?abstractid=5128927&mirid=1&type=2 ([SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5128927))
- Technical Survey of RL for LLMs（arXiv）：https://arxiv.org/abs/2507.04136 ｜ PDF：https://arxiv.org/pdf/2507.04136 ([arXiv](https://arxiv.org/abs/2507.04136))
- Agentic RL（arXiv）：https://arxiv.org/abs/2509.02547 ｜ PDF：https://arxiv.org/pdf/2509.02547 ([arXiv](https://arxiv.org/abs/2509.02547))
- LRM（arXiv）：https://arxiv.org/abs/2501.09686 ｜ PDF：https://arxiv.org/pdf/2501.09686 ([arXiv](https://arxiv.org/abs/2501.09686))

